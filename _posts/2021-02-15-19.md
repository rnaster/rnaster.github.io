---
title: Week04 Day16
tags:
  - BoostCamp-ai-tech
  - Week04
  - NLP
  - Distributional semantics
  - Word2Vec
  - CBOW
  - Skip-gram
  - GloVe
---

4주차는 자연어 처리에 대해 배운다.  

### NLP(Natural Language Processing)
컴퓨터가 인간 언어를 적절히 이해하고 생성하는데 목표를 둔 NLP는 DNN의 발전과 함께 인공지능의 중요한 application 중 하나이다.  
주요 학회는 **ACL, EMNLP, NAACL** 등이 있다.  
NLP의 주요 task는 다음과 같다.  
- low-level parsing  
  - tokenization  
  문장, 문서 형태로 된 자연어 텍스트를 모델이 이해할 수 있는 최소 의미 단위인 "token"이라는 단위로 분리하는 과정이다.  
  ex) i like bread -> i, like, bread
  - stemming  
  단어의 의미를 내포하는 부분 추출.  
  ex) allowance -> allow, formalize -> formal
- word, phrase level
  - NER(Named Entity Recognition)  
  > a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.[^1]  

  정의만 보면 다소 이해가 안됐다.  
  예시를 보니 이해가 됐다.  
  `Jim bought 300 shares of Acme Corp. in 2006.`  
  위와 같은 문장이 있다면 아래와 같이 감지하고 분류한다.  
  <code>[Jim]<sub>Person</sub> bought 300 shares of [Acme Corp.]<sub>Organization</sub> in [2006]<sub>Time</sub></code>  
  각 단어를 살펴보며 미리 정의된 class(여기서는 `Person, Organization, Time`) 중 어울리는 class로 분류한다.  
  - POS(Part Of Speech) tagging  
  문맥에 따라 단어의 품사를 분류한다.  
  ex) i can fly(verb) vs that\`s fly(noun)
  - phrase chunking  
  문장을 구(phrase, 명사구, 동사구 등)단위로 분리한다.  
  - dependency parsing
  - coreference resolution
- sentence level
  - sentiment analysis
  - machine translation
- multi-sentence and paragraph level
  - entailment prediction
  - question answering
  - dialog systems
  - summarization

#### NLP Trend
- Word2Vec, GloVe과 같은 기술로 단어들을 벡터로 변환할 수 있다.
- RNN 계열의 모델들은 NLP의 주요 아키텍쳐이다.
- 최근에 attention module과 transformer model의 등장으로 NLP task의 전반적인 성능을 향상시켜서 RNN을 대체하고 있다.
- 원래 transformer model은 기계번역을 위해 개발되었다.  
딥러닝 이전 기계번역은 전문가들이 만든 rule-based translation이었다.  
이 방법은 언어의 다양한 패턴을 모두 고려하는 것은 어려웠다.  
이러한 상황에서 transformer의 등장은 기계번역의 성능을 높이는데 기여했다.  
요즘은 기계번역 이외 분야에서 transformer model을 사용하고 있다.  
- transformer가 나오기 전에 서로 다른 NLP task에 대한 모델이 각각 존재했다.  
하지만 이후에는 범용적인 모델을 만들어서 여러 task에 적용할 수 있게 되었다.  
어떤 domain에서 얻은 정보로 관련있는 다른 domain의 task를 해결하는 방법을 transfer learning이라 한다.[^2]  

### what is "meaning"
앞서 NLP는 컴퓨터가 인간의 언어를 이해하는 것이라 소개했다.  
그럼 이해하기 위해 우리가 사용하는 언어의 의미를 알아야 하지 않을까?  
예를 들어 `사과`라는 말을 이해하기 위해 의미를 알아야 할 것이다.  
그럼 `의미(meaning)`이라는 것은 무엇일까?  
> In the philosophy of language, metaphysics, and metasemantics, meaning "is a relationship between two sorts of things: signs and the kinds of things they intend, express, or signify".[^3]  

철학적으로 `의미(meaning)`은 두 종류의 것들간의 관계이다: `signs`과 그것들이 의도, 표현 혹은 의미하는 것들의 종류들.  

위 정의를 보고 `의미(meaning)`이 뭔지 이해가 잘 안된다...ㅜㅜ  
내가 해석을 잘 못하는 건가...  
좀 더 생각해보자...  

### Distributional semantics
나중에 추가...

### Word2Vec
나중에 추가...

### CBOW(Continuous Bag of Words)
나중에 추가...

### Skip-gram
나중에 추가...

### GloVe(Global Vectors for Word Representation)
나중에 추가...

### distance metric
나중에 추가...


[^1]: [위키피디아 - Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)  
[^2]: [위키피디아 - transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)  
[^3]: [위키피디아 - meaning](https://en.wikipedia.org/wiki/Meaning_(philosophy))  
