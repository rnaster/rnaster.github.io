---
title: Week04 Day16
tags:
  - BoostCamp-ai-tech
  - Week04
  - NLP
  - Distributional hypothesis
  - Word2Vec
  - CBOW
  - Skip-gram
  - GloVe
---

4주차는 자연어 처리에 대해 배운다.  

### NLP(Natural Language Processing)
컴퓨터가 인간 언어를 적절히 이해하고 생성하는데 목표를 둔 NLP는 DNN의 발전과 함께 인공지능의 중요한 application 중 하나이다.  
주요 학회는 **ACL, EMNLP, NAACL** 등이 있다.  
NLP의 주요 task는 다음과 같다.  
- low-level parsing  
  - tokenization  
  문장, 문서 형태로 된 자연어 텍스트를 모델이 이해할 수 있는 최소 의미 단위인 "token"이라는 단위로 분리하는 과정이다.  
  ex) i like bread -> i, like, bread
  - stemming  
  단어의 의미를 내포하는 부분 추출.  
  ex) allowance -> allow, formalize -> formal
- word, phrase level
  - NER(Named Entity Recognition)  
  > a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.[^1]  

  정의만 보면 다소 이해가 안됐다.  
  예시를 보니 이해가 됐다.  
  `Jim bought 300 shares of Acme Corp. in 2006.`  
  위와 같은 문장이 있다면 아래와 같이 감지하고 분류한다.  
  <code>[Jim]<sub>Person</sub> bought 300 shares of [Acme Corp.]<sub>Organization</sub> in [2006]<sub>Time</sub></code>  
  각 단어를 살펴보며 미리 정의된 class(여기서는 `Person, Organization, Time`) 중 어울리는 class로 분류한다.  
  - POS(Part Of Speech) tagging  
  문맥에 따라 단어의 품사를 분류한다.  
  ex) i can fly(verb) vs that\`s fly(noun)
  - phrase chunking  
  문장을 구(phrase, 명사구, 동사구 등)단위로 분리한다.  
  - dependency parsing
  - coreference resolution
- sentence level
  - sentiment analysis
  - machine translation
- multi-sentence and paragraph level
  - entailment prediction
  - question answering
  - dialog systems
  - summarization

#### NLP Trend
- Word2Vec, GloVe과 같은 기술로 단어들을 벡터로 변환할 수 있다.
- RNN 계열의 모델들은 NLP의 주요 아키텍쳐이다.
- 최근에 attention module과 transformer model의 등장으로 NLP task의 전반적인 성능을 향상시켜서 RNN을 대체하고 있다.
- 원래 transformer model은 기계번역을 위해 개발되었다.  
딥러닝 이전 기계번역은 전문가들이 만든 rule-based translation이었다.  
이 방법은 언어의 다양한 패턴을 모두 고려하는 것은 어려웠다.  
이러한 상황에서 transformer의 등장은 기계번역의 성능을 높이는데 기여했다.  
요즘은 기계번역 이외 분야에서 transformer model을 사용하고 있다.  
- transformer가 나오기 전에 서로 다른 NLP task에 대한 모델이 각각 존재했다.  
하지만 이후에는 범용적인 모델을 만들어서 여러 task에 적용할 수 있게 되었다.  
어떤 domain에서 얻은 정보로 관련있는 다른 domain의 task를 해결하는 방법을 transfer learning이라 한다.[^2]  

### what is "meaning"
이 부분은 [cs224n - lecture 1](http://web.stanford.edu/class/cs224n/)을 참고했다.  

앞서 NLP는 컴퓨터가 인간의 언어를 이해하는 것이라 소개했다.  
그럼 이해하기 위해 우리가 사용하는 언어의 의미를 알아야 하지 않을까?  
예를 들어 `사과`라는 말을 이해하기 위해 의미를 알아야 할 것이다.  
그럼 `의미(meaning)`이라는 것은 무엇일까?  
> In the philosophy of language, metaphysics, and metasemantics, meaning "is a relationship between two sorts of things: signs and the kinds of things they intend, express, or signify".[^3]  

철학적으로 `의미(meaning)`은 두 종류의 것들간의 관계이다: `signs`과 그것들이 의도, 표현 혹은 의미하는 것들의 종류들.  
예를 들어 `계곡`이라는 단어를 생각해보면 `시원하다`, `여름`, `휴가` 등이 떠오른다.  
이처럼 어떤 단어의 의미라는 것은 관련해서 떠오르는 이미지나 단어 혹은 기호 등이 아닐까 생각된다.  
사실 위 정의를 보고 철학적으로 `의미(meaning)`이 뭔지 이해가 잘 안된다...ㅜㅜ  
내가 해석을 잘 못하는 건가...  
좀 더 생각해보자...  
일단 나는 `의미(meaning)`를 `어떤 단어를 생각하면 떠오르는 생각이나 상징하는 기호` 라고 이해할 것이다.  
컴퓨터에서는 `의미(meaning)`를 어떻게 이용할까?  
고전적인 방법으로 유의어 사전을 만드는 것이다.  
`good`이라는 단어의 품사별 유의어를 사용하여 의미를 나타낼 수 있을 것이다.  
이렇게 사전을 만드는 것은 다음과 같은 문제가 있다.  
- 문맥에 따라 더 적합한 단어를 선택할 수 없다.  
즉, 뉘앙스를 구분할 수 없다.
- 새로운 단어가 생기면 업데이트가 느리다.  
사람들의 노동력이 많이 필요하다.  
- 유의어라고 분류하는 것은 주관적이다.
- `good`과 `positive`, `proficient` 간의 유사도를 구할 수 없다.  

만약 단어간 유사도를 구할 수 있다면 한 단어와 유사한 단어를 사람이 직접 등록할 필요없고 문맥에 따라 더 적합한 단어를 선택할 수 있지 않을까?  
그럼 유사도를 어떻게 구할까?  
그전에 단어를 컴퓨터 안에 어떻게 표현을 할까?  
먼저, one-hot vector로 표현해보자.  
벡터의 차원은 vocabulary size가 된다.  
예를 들어 `사과, 오렌지, 바나나, 포도`가 있을때 vocabulary size는 4이다.  
그럼 각 단어들을 one-hot vector로 바꾸면 다음과 같다.  
`사과` = `[1, 0, 0, 0]`  
`오렌지` = `[0, 1, 0, 0]`  
`바나나` = `[0, 0, 1, 0]`  
`포도` = `[0, 0, 0, 1]`  
`사과`와 `오렌지`의 유사도를 구해보면, 이때 유사도는 내적(dot product)를 사용한다, `0`이다.  
두 벡터의 내적이 `0`이라는 것은 두 벡터가 수직이라는 의미이다.  
이외에 다른 두 단어의 유사도를 구해도 모두 `0`이 나온다.  
즉, 단어를 one-hot vector로 표현하면 유사도를 구할 수 없다.  
그래서 등장한 방법은 벡터 자체에 단어 간 유사도를 녹아들게(?) 하는 것이다.  
다음은 단어 벡터에 유사도를 녹아들게 하는 생각의 배경이 되는 가정을 소개한다.  

#### Distributional hypothesis
> The distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings.[^4]  

같은 문맥에서 사용되고 발생되는 단어들은 비슷한 의미를 지니는 경향이 있다.  
이 가정의 근본적인 아이디어는 [J. R. Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth)에 의한 `a word is characterized by the company it keeps`이다.  
즉, 단어의 의미는 근처에 자주 나타나는 단어에 의해 부여된다.  
예를 들어 `나는 친구에게 사과를 했다. 친구는 나의 사과를 받아줬다.` 라는 문장에서 `사과`의 의미는 `자기의 잘못을 인정하고 용서를 빌다.`[^5] 이다.  
이 의미는 주변 단어 혹은 문맥을 통해 부여된 것이다.  
그래서 각 단어를 `[0.9120, -0.4132, 1.2344, -0.0043]`와 같은 dense vector로 변환하여 비슷한 문맥에서 나타나는 단어끼리 유사하도록 만들어 볼 것이다.  
이때 이런 벡터들을 `word embedding` 혹은 `word representation`라 부른다.  
다음은 단어 간 유사도를 학습하는 방법을 소개한다.  

### Word2Vec
나중에 추가...

### CBOW(Continuous Bag of Words)
나중에 추가...

### Skip-gram
나중에 추가...

#### negative sampling
나중에 추가...

### GloVe(Global Vectors for Word Representation)
나중에 추가...

### distance metric
나중에 추가...

### arithmetic property for word embedding
나중에 추가...


[^1]: [위키피디아 - Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)  
[^2]: [위키피디아 - transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)  
[^3]: [위키피디아 - meaning](https://en.wikipedia.org/wiki/Meaning_(philosophy))  
[^4]: [위키피디아 - Distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)  
[^5]: [네이버 국어사전](https://ko.dict.naver.com/ko/entry/koko/e0d6d9d4055442f08e406dc602cabe25)  
