---
title: Week02 Day07
tags:
  - BoostCamp-ai-tech
  - Week02
  - differentiation
  - stochastic gradient descent
---

### 미분
>어떤 함수의 정의역 속 각 점에서 함숫값의 변화량과 독립 변숫값의 변화량 비의 극한 혹은 극한들로 치역이 구성되는 새로운 함수다.  
어떤 함수의 미분 계수 또는 순간 변화율을 구하는 것을 의미하며 미분 계수는 독립 변수 x의 증분에 관한 함숫값 ƒ(x)의 증분의 비가 한없이 일정한 값에 가까워질 때 그 일정한 값, 즉 함수에서 변수 x값의 변화량에 관한 함숫값 ƒ(x)의 변화량 비가 한없이 일정한 값에 가까워질 때 그 일정한 값 dy/dx로 나타낸다.  
미분은 비선형 함수를 선형함수로 근사적으로 나타내려는 시도이다.[^1]  

위키피디아 정의는 다소 어렵다.  
미분을 단순하게 `순간변화율` 혹은 `접선의 기울기` 혹은 강의에서 언급했듯이 `변수의 움직임에 따른 함수값의 변화를 측정`이라고 생각하기로 했다.  
한 점에서의 접선의 기울기를 알면 어느 방향으로 이동해야 함수값이 증가하는지 알 수 있다.  
예를 들어 `접선의 기울기`가 양수이면 `x`값이 증가하면 함수값이 증가하고 `접선의 기울기`가 음수일때 `x`값이 증가하면 함수값은 감소한다.  

### gradient descent
> Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.  
The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent.  
Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.  
When the function F is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution.[^2]  

gradient descent는 1계 도함수로 주변 최소값을 찾는 반복적인 최적화 알고리즘이다.  
이 방법은 현재 지점에서의 gradient의 반대방향으로 반복적으로 이동한다.  
왜냐하면 반대방향이 함수 값이 가장 빠르게 감소한다.  
역으로 gradient 방향으로 움직이는 것은 함수의 국소 최대값에 이른다, 이것을 `gradient ascent`라 한다.  
함수 F가 `convex`일때 모든 국소 최소값은 전역 최소값이며 이런 경우 gradient descent는 global solution에 수렴할 수 있다.  

### stochastic gradient descent
`gradient descent`의 문제는 non-convex 함수에서 최적화된 값을 보장하기 어렵다.  
이를 개선한 방법이 `stochastic gradient descent`이다.  
`stochastic`의 의미는 `확률적인`인데 이 방법은 행렬 `X` 중 일부의 행벡터만 사용하기 때문이다.  
그 외 차이점은 없다.  


[^1]: [위키피디아 - 미분](https://ko.wikipedia.org/wiki/%EB%AF%B8%EB%B6%84)  
[^2]: [위키피디아 - gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)  
