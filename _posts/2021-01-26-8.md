---
title: Week02 Day07
tags:
  - BoostCamp-ai-tech
  - Week02
  - differentiation
  - stochastic gradient descent
---

### 미분
>어떤 함수의 정의역 속 각 점에서 함숫값의 변화량과 독립 변숫값의 변화량 비의 극한 혹은 극한들로 치역이 구성되는 새로운 함수다.  
어떤 함수의 미분 계수 또는 순간 변화율을 구하는 것을 의미하며 미분 계수는 독립 변수 x의 증분에 관한 함숫값 ƒ(x)의 증분의 비가 한없이 일정한 값에 가까워질 때 그 일정한 값, 즉 함수에서 변수 x값의 변화량에 관한 함숫값 ƒ(x)의 변화량 비가 한없이 일정한 값에 가까워질 때 그 일정한 값 dy/dx로 나타낸다.  
미분은 비선형 함수를 선형함수로 근사적으로 나타내려는 시도이다.  [^1]  

위키피디아 정의는 다소 어렵다.  
미분을 단순하게 `순간변화율` 혹은 `접선의 기울기` 혹은 강의에서 언급했듯이 `변수의 움직임에 따른 함수값의 변화를 측정`이라고 생각하기로 했다.  
한 점에서의 접선의 기울기를 알면 어느 방향으로 이동해야 함수값이 증가하는지 알 수 있다.  
예를 들어 `접선의 기울기`가 양수이면 `x`값이 증가하면 함수값이 증가하고 `접선의 기울기`가 음수일때 `x`값이 증가하면 함수값은 감소한다.  

### gradient descent
함수값을 증가시키기 위해 미분값을 `x`에 더하는 것을 `gradient ascent`라 하고 감소시키기 위해 미분값을 `x`에 빼는 것을 `gradient descent`라 한다.  
컴퓨터로 계산할때는 미분값이 정확히 0이 나오기 힘들기 때문에 `접선의 기울기`가 적당히 작은 값보다 작으면 종료한다.  
정리하자면 함수값은 당연하게도 `x`에 의해 결정된다.  
가장 작은 혹은 큰 함수값을 찾기 위해 임의의 `x`를 정하고 `접선의 기울기`를 구하여 `x`에 더하거나 빼서 바꾼다.  
이때 `접선의 기울기`에 1보다 작은 값을 곱할 수 있는데 이 값을 학습률(learning rate)라 부른다.  
이유는 `x`를 급격히 변화시키지 않고 조금씩 변화시켜서 안정적으로 원하는 함수값에 도달하기 위함이다.  
이러한 과정을 `최적화(optimization)`이라 한다.  

벡터함수에서는 `접선의 기울기` 대신 `gradient`를 사용한다.  
`gradient`는 한 점에서 가장 빠르게 변화하는 방향을 나타내는 벡터이다.  
선형모델인 경우 `Moore–Penrose inverse matrix`를 구하여 최적화를 할 수 있지만, 비선형모델에는 이를 활용하기 힘들다.  
그래서 일반적으로도 쓰일 수 있는 `gradient descent` 방법을 사용한다.  
그래도 `gradient descent` 역시 만능은 아니다.  
함수가 볼록(convex)라면 최적의 값에 도달할 수 있지만 볼록하지 않다면(non-convex) 보장하기 어렵다.  

### stochastic gradient descent
`gradient descent`의 문제는 non-convex 함수에서 최적화된 값을 보장하기 어렵다.  
이를 개선한 방법이 `stochastic gradient descent`이다.  
`stochastic`의 의미는 `확률적인`인데 이 방법은 행렬 `X` 중 일부의 행벡터만 사용하기 때문이다.  
그 외 차이점은 없다.  


[^1]: [위키피디아 - 미분](https://ko.wikipedia.org/wiki/%EB%AF%B8%EB%B6%84)
