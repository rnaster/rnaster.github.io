---
title: Week09 Day43
tags:
  - BoostCamp-ai-tech
  - Week09
  - torch.nn.Module
  - daily mission
  - To do
---

### torch.nn.Module
`torch.nn.Module`은 pytorch에서 모델을 만드는데 기본이 되는 class이며 상속하여 원하는 모델을 구현한다.  
이 class에 대해서 좀 더 알아봐야겠다.

나중에 추가...

### daily mission
```python
import torch.nn as nn


class BasicBlock(nn.Module):
    def __init__(self, in_channel: int, out_channels: [int, int]):
        super(BasicBlock, self).__init__()
        out1, out2 = out_channels
        self.conv1 = nn.Conv2d(in_channel, out1, 1, 1)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out1, out2, 3, 1, 1)
        self.relu2 = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        return self.relu2(x)


class DailyMission(nn.Module):
    def __init__(self, in_channel):
        super(DailyMission, self).__init__()
        self.conv1 = nn.Conv2d(in_channel, 32, 3, 1, 1)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(32, 64, 3, 2, 1)
        self.relu2 = nn.ReLU(inplace=True)
        self.block1 = self.multiple_blocks(64, (32, 64), 1)
        self.conv3 = nn.Conv2d(64, 128, 3, 2, 1)
        self.relu3 = nn.ReLU(inplace=True)
        self.block2 = self.multiple_blocks(128, (64, 128), 2)
        self.conv4 = nn.Conv2d(128, 256, 3, 2, 1)
        self.relu4 = nn.ReLU(inplace=True)
        self.block3 = self.multiple_blocks(256, (128, 256), 8)
        self.conv5 = nn.Conv2d(256, 512, 3, 2, 1)
        self.relu5 = nn.ReLU(inplace=True)
        self.block4 = self.multiple_blocks(512, (256, 512), 8)
        self.conv6 = nn.Conv2d(512, 1024, 3, 2, 1)
        self.relu6 = nn.ReLU(inplace=True)
        self.block5 = self.multiple_blocks(1024, (512, 1024), 4)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(1024, 1000)

    def forward(self, x):
        features = nn.Sequential(
            self.conv1, self.relu1,
            self.conv2, self.relu2,
            self.block1,
            self.conv3, self.relu3,
            self.block2,
            self.conv4, self.relu4,
            self.block3,
            self.conv5, self.relu5,
            self.block4,
            self.conv6, self.relu6,
            self.block5,
            self.avg_pool
        )
        x = features(x).squeeze()
        return self.fc(x)

    def multiple_blocks(self, in_channel, out_channels, iter_nums):
        blocks = []
        out1, out2 = out_channels
        for _ in range(iter_nums):
            blocks.append(BasicBlock(in_channel, (out1, out2)))
            in_channel = out2
        return nn.Sequential(*blocks)
```

### model search
어제에 이어서 모델을 탐색하는 과정을 계속했다.  
`Inception_V3`을 학습할 때 문제가 발생했다.  
이전 모델들은 `auxiliary classifier`가 없었다.  
그래서 loss를 한번만 계산했는데 `Inception_V3`에서 문제가 발생한 것이다.  
심지어 `GoogLeNet`은 `auxiliary classifier`가 2개이다.  
이 문제를 해결하고 다시 학습하고 있다.  
혹시 다른 모델에서 예상하지 못한 문제가 발생할 수 있어서 1개의 batch에 대해서 모두 학습해봤다.  
다행히 문제가 없어서 어제 중단한 모델 탐색을 하는 중이다.  

아래는 매 epoch 마다 train accuracy를 기록한 것이다.  
각 모델마다 epoch 5로 학습시켰다.  
이 중에서 최종 accuracy가 가장 높은 모델을 먼저 학습할 예정이다.  
<details><summary> 결과 </summary>

[2021-03-31 07:59:06,698] start <br>
[2021-03-31 08:03:05,864] [MNASNet] [epoch: 0] [acc: 0.7860317460317461] <br>
[2021-03-31 08:07:07,714] [MNASNet] [epoch: 1] [acc: 0.8421164021164022] <br>
[2021-03-31 08:10:58,665] [MNASNet] [epoch: 2] [acc: 0.8902645502645503] <br>
[2021-03-31 08:14:54,243] [MNASNet] [epoch: 3] [acc: 0.9187830687830688] <br>
[2021-03-31 08:18:49,666] [MNASNet] [epoch: 4] [acc: 0.9395767195767196] <br>
[2021-03-31 08:28:10,966] [wide_resnet50_2] [epoch: 0] [acc: 0.5248677248677248] <br>
[2021-03-31 08:37:29,048] [wide_resnet50_2] [epoch: 1] [acc: 0.7243915343915344] <br>
[2021-03-31 08:46:49,462] [wide_resnet50_2] [epoch: 2] [acc: 0.7870370370370371] <br>
[2021-03-31 08:56:09,030] [wide_resnet50_2] [epoch: 3] [acc: 0.7973015873015873] <br>
[2021-03-31 09:05:27,983] [wide_resnet50_2] [epoch: 4] [acc: 0.8365079365079365] <br>
[2021-03-31 09:13:24,620] [resnet50_32x4d] [epoch: 0] [acc: 0.6677248677248677] <br>
[2021-03-31 09:21:19,925] [resnet50_32x4d] [epoch: 1] [acc: 0.785978835978836] <br>
[2021-03-31 09:29:11,478] [resnet50_32x4d] [epoch: 2] [acc: 0.8133862433862434] <br>
[2021-03-31 09:37:04,509] [resnet50_32x4d] [epoch: 3] [acc: 0.8745502645502645] <br>
[2021-03-31 09:44:55,944] [resnet50_32x4d] [epoch: 4] [acc: 0.8826984126984126] <br>
[2021-03-31 09:48:51,717] [MobileNetV2] [epoch: 0] [acc: 0.8024338624338624] <br>
[2021-03-31 09:52:46,848] [MobileNetV2] [epoch: 1] [acc: 0.8482010582010582] <br>
[2021-03-31 09:56:42,491] [MobileNetV2] [epoch: 2] [acc: 0.8804232804232804] <br>
[2021-03-31 10:00:38,697] [MobileNetV2] [epoch: 3] [acc: 0.9076190476190477] <br>
[2021-03-31 10:04:35,877] [MobileNetV2] [epoch: 4] [acc: 0.8942857142857142] <br>
[2021-03-31 10:08:31,930] [ShuffleNetV2] [epoch: 0] [acc: 0.7905291005291005] <br>
[2021-03-31 10:12:25,686] [ShuffleNetV2] [epoch: 1] [acc: 0.8438624338624339] <br>
[2021-03-31 10:16:21,036] [ShuffleNetV2] [epoch: 2] [acc: 0.8978835978835978] <br>
[2021-03-31 10:20:16,833] [ShuffleNetV2] [epoch: 3] [acc: 0.9004232804232805] <br>
[2021-03-31 10:24:11,477] [ShuffleNetV2] [epoch: 4] [acc: 0.9301587301587302] <br>
[2021-03-31 10:42:07,653] [GoogLeNet] [epoch: 0] [acc: 0.6864550264550264] <br>
[2021-03-31 10:46:05,083] [GoogLeNet] [epoch: 1] [acc: 0.7838624338624338] <br>
[2021-03-31 10:50:08,035] [GoogLeNet] [epoch: 2] [acc: 0.8464550264550265] <br>
[2021-03-31 10:54:09,931] [GoogLeNet] [epoch: 3] [acc: 0.8657142857142858] <br>
[2021-03-31 10:58:12,302] [GoogLeNet] [epoch: 4] [acc: 0.8938624338624339] <br>
[2021-03-31 11:03:50,418] [Inception3] [epoch: 0] [acc: 0.8006878306878307] <br>
[2021-03-31 11:09:26,716] [Inception3] [epoch: 1] [acc: 0.894021164021164] <br>
[2021-03-31 11:15:03,760] [Inception3] [epoch: 2] [acc: 0.8871428571428571] <br>
[2021-03-31 11:20:41,002] [Inception3] [epoch: 3] [acc: 0.913968253968254] <br>
[2021-03-31 11:26:16,120] [Inception3] [epoch: 4] [acc: 0.94005291005291] <br>
[2021-03-31 11:39:15,231] [DenseNet] [epoch: 0] [acc: 0.6541798941798942] <br>
[2021-03-31 11:52:12,255] [DenseNet] [epoch: 1] [acc: 0.7678306878306879] <br>
[2021-03-31 12:05:10,596] [DenseNet] [epoch: 2] [acc: 0.8386772486772487] <br>
[2021-03-31 12:18:07,517] [DenseNet] [epoch: 3] [acc: 0.8617989417989418] <br>
[2021-03-31 12:31:06,899] [DenseNet] [epoch: 4] [acc: 0.8793121693121693] <br>
[2021-03-31 12:35:02,761] [SqueezeNet] [epoch: 0] [acc: 0.27470899470899474] <br>
[2021-03-31 12:38:54,517] [SqueezeNet] [epoch: 1] [acc: 0.29164021164021164] <br>
[2021-03-31 12:42:47,962] [SqueezeNet] [epoch: 2] [acc: 0.4216931216931217] <br>
[2021-03-31 12:46:38,705] [SqueezeNet] [epoch: 3] [acc: 0.4712169312169312] <br>
[2021-03-31 12:50:29,553] [SqueezeNet] [epoch: 4] [acc: 0.47603174603174603] <br>
[2021-03-31 12:54:21,069] [resnet18] [epoch: 0] [acc: 0.7430687830687831] <br>
[2021-03-31 12:58:12,932] [resnet18] [epoch: 1] [acc: 0.8288888888888889] <br>
[2021-03-31 13:02:05,006] [resnet18] [epoch: 2] [acc: 0.8612169312169312] <br>
[2021-03-31 13:05:57,355] [resnet18] [epoch: 3] [acc: 0.8813227513227513] <br>
[2021-03-31 13:09:49,905] [resnet18] [epoch: 4] [acc: 0.905079365079365] <br>
[2021-03-31 13:13:41,547] [AlexNet] [epoch: 0] [acc: 0.29164021164021164] <br>
[2021-03-31 13:17:33,129] [AlexNet] [epoch: 1] [acc: 0.45] <br>
[2021-03-31 13:21:25,370] [AlexNet] [epoch: 2] [acc: 0.5170370370370371] <br>
[2021-03-31 13:25:16,927] [AlexNet] [epoch: 3] [acc: 0.4674074074074074] <br>
[2021-03-31 13:29:12,901] [AlexNet] [epoch: 4] [acc: 0.6394708994708995] <br>
[2021-03-31 13:39:43,225] [VGG] [epoch: 0] [acc: 0.21169312169312168] <br>
[2021-03-31 13:50:12,115] [VGG] [epoch: 1] [acc: 0.20322751322751323] <br>

</details>

### To do
- resnet, inception3, shufflenet, MNASNet, efficientNet 학습 해보기
  - epoch 늘리고 cross validation 사용
- transform 사용하기
  - centercrop, randomcrop, randomhorizonflip
- labeling imbalanced 문제 고민해보기
- mis-labeling 고쳐서 학습 해보기

### trivia
remote server의 process를 중단하는 과정에서 실수로 jupyter-lab을 종료했다.  
그랬더니 서버에 연결이 안 됐다...  
식은 땀이 났다.  
다행히 기존 서버를 제거하고 새로운 서버를 할당 받았다.  
기존에 하던 학습이 다 날라갔지만 중요한 것을 알아서 다행이다...  

또 다시 서버를 날렸다.  
이번에는 pycharm으로 내 컴퓨터의 코드 저장된 곳과 서버의 코그 저장할 곳을 연결하려고 하는데 뭘 잘못 건드려서 `/` 디렉토리에 있는 파일들을 다 지웠다...  
이런 일을 두번 반복했고 다행히 뭐가 문제인지 파악했다.  
pycharm 설정을 잘못 건드렸다.  
후... 이것 때문에 진짜 힘들었다.  
오늘 해야할 것을 제대로 못했다.  
