---
title: Week03 Day15
tags:
  - BoostCamp-ai-tech
  - Week03
  - generative model
  - autoencoder
  - VAE
  - GAN
mathjax: true
---

### generative model
![](/assets/images/27.PNG)  
> What I cannot create, I do not understand - Richard Feynman -  

생성모델(generative model)의 목적이 뭘까?  
이름에서 알 수 있듯이 무언가 생성하는 모델?  
맞는 말이지만 이게 생성모델의 목적의 전부가 아니다.  
강아지 사진이 주어졌다고 하자.  
그러면 다음과 같은 모델들을 학습할 수 있다.  
- 강아지처럼 보이는 이미지를 생성할 수 있는 확률분포.  
- 확률밀도함수에 어떤 이미지를 입력으로 하여 높은 값을 얻는다면 강아지일 가능성이 크다.  
이 경우 explicit model이라고도 불린다.  
- 주어진 강아지 사진들의 공통적인 특징을 학습할 수 있다.  

단순히 generation만 가능하다면 implicit model이라 한다.  
엄밀하게 말하면 generative model은 discriminative model을 포함한다.  

![](/assets/images/28.png)[^1]  
위 사진은 실제 사람들의 숫자 필기체이다.  
이 사진들은 여러개의 pixel로 이루어 지며 하나의 pixel은 0 혹은 1 값을 가지는 binary pixel이다.  
한 이미지의 pixel이 n개이고 하나의 pixel을 $x_i$ 라 한다면 그 이미지가 나타날 확률은 $P(x_1,\ x_2,\ ...,\ x_n)$ 이다.  
그럼 이 경우 필요한 parameter의 수는 $2^n-1$ 이다.  
pixel 수가 많아질수록 기하급수적으로 증가한다.  
만약 pixel 간 서로 독립이라면 $P(x_1,\ x_2,\ ...,\ x_n) = P(x_1)P(x_2)...P(x_n)$ 이다.  
이 경우 parameter의 갯수는 $n$ 이다.  
독립이라는 조건에 의해 parameter 수가 급격히 줄었다.  
하지만 pixel 간 독립은 사실상 성립하기 힘들다.  
사진의 한 pixel은 주위 pixel에 영향을 받는다.  
따라서 독립은 매우 이상적이고 까다로운 조건이다.  
하지만 조건을 완화한다면 parameter를 줄일 수 있다.  

### conditional independence
위에서의 $P(x_1,\ x_2,\ ...,\ x_n)$ 를 chain rule에 의해 아래와 같이 쓸 수 있다.  

$$
P(x_1,\ x_2,\ ...,\ x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,\ x_2)...P(x_n|x_1,\ ...,\ x_{n-1})
$$  
$P(x_1)$ 의 parameter 개수는 1개이다.  
$P(x_2|x_1)$ 의 parameter 수는 $P(x_2|x_1 = 0), P(x_2|x_1 = 1)$ 로 2개이다.  
비슷하게 $P(x_3|x_1,\ x_2)$ 는 4개이다.  
그래서 모든 parameter의 수는 1 + 2 + 4 + ... + $2^{n-1} = 2^n-1$ 이다.  
chain rule로 parameter 수는 바뀌지 않았다.  

여기에 $x_i$ 가 주어질때 $x_{i+1}$ 은 $x_1,\ ...,\ x_{i-1}$ 와 독립이라는 조건을 추가해보자.  
이를 조건부독립이라 하며 이를 적용하면 아래와 같이 쓸 수 있다.  

$$
P(x_1,\ x_2,\ ...,\ x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,\ x_2)...P(x_n|x_1,\ ...,\ x_{n-1})\\
\qquad = P(x_1)P(x_2|x_1)p(x_3|x_2)...P(x_n|x_{n-1})
$$  
확률변수 $x_i$ 가 바로 이전 $x_{i-1}$ 에만 영향을 받는 조건을 **markov assumption** 이라 한다.  
이때 parameter의 수는 $2n-1$ 이다.  
이런 모델을 **auto-regressive model** 이라 한다.  
정리해보면 최초 $P(x_1,\ x_2,\ ...,\ x_n)$ 에서 어떤 조건을 주느냐에 따라 parameter 수를 줄일 수 있었다.  

### auto-regressive model
**mnist** 는 28 x 28 binary pixel이다.  
우리는 이 이미지들에 대한 확률분포 $P(x) = P(x_1,\ ...,\ x_{784})$ 를 알고 싶다.  
그럼 parameter를 어떻게 구해야 할까?  
위에서 보였듯이 chain rule에 의해 $P(x) = P(x_1,\ ...,\ x_{784}) = P(x_1)P(x_2|x_1)P(x_3|x_1,\ x_2)...P(x_{784}|x_1,\ ...,\ x_{783})$ 으로 바꿀 수 있다.  
이런 모델을 auto-regressive model이라 한다.  
근데 pixel 간의 순서를 어떻게 정해야 될까?  

### NADE(Neural Auto-regressive Density Estimator)
나중에 추가

### Pixel RNN
나중에 추가

#### KL-divergence(Kullback-Leibler divergence)
나중에 추가

### autoencoder [^2]
나중에 추가

### VAE(Variational Auto-Encoder) [^3]
**variational inference(VI)** 란 직접 계산하기 힘든 혹은 복잡한 형태의 사후확률분포(posterior distribution)와 비슷한 variational distribution을 찾는 과정을 말한다.  
여기서 variational distribution은 사후확률분포보다 간단한 형태이다, 예를 들어 gaussian distribution.  
이 분포를 찾기 위해 사후분포와 KL-divergence를 최소화하여 근사시킨다.  


### GAN(Generative Adversarial Network)
나중에 추가


[^1]: [위키피디아 - mnist](https://en.wikipedia.org/wiki/MNIST_database)  
[^2]: [오토인코더의 모든 것-1](https://youtu.be/o_peo6U7IRM)  
[^3]: [오토인코더의 모든 것-2](https://youtu.be/rNh2CrTFpm4)  
